name: Generate RayCluster YAML via Helm
description: Renders the RayCluster Helm chart into Kubernetes manifests without installing.

inputs:
- name: namespace
  type: String
  description: Kubernetes namespace for the RayCluster.
- name: release_name
  type: String
  description: Helm release name for the RayCluster.
- name: num_nodes
  type: Integer
  description: Number of Ray worker nodes.
- name: gpus_per_node
  type: Integer
  description: Number of GPUs per Ray worker node.

outputs:
- name: rendered_yaml
  description: Path to the rendered Kubernetes manifests.

implementation:
  container:
    image: alpine/helm:latest
    command:
    - sh
    - -c
    - |
      namespace=$0
      release_name=$1
      num_nodes=$2
      gpus_per_node=$3
      rendered_yaml=$4
      set -e
      echo "Adding KubeRay Helm repository..."
      helm repo add kuberay https://ray-project.github.io/kuberay-helm/
      helm repo update

      echo "Creating temporary values.yaml file..."
      cat <<EOF > /tmp/values.yaml
      image:
        tag: 2.41.0-gpu
      head:
        annotations:
          sidecar.istio.io/inject: "false"
        containerEnv:
          - name: RAY_GRAFANA_HOST
            value: http://kube-prometheus-stack-1745905925-grafana.prometheus.svc.cluster.local:80
          - name: RAY_PROMETHEUS_HOST
            value: http://prometheus-operated.prometheus.svc.cluster.local:9090
          - name: RAY_PROMETHEUS_NAME
            value: Prometheus
          - name: RAY_GRAFANA_IFRAME_HOST
            value: http://172.18.52.185:31529
        resources:
          limits:
            cpu: 4
            memory: 16G
            nvidia.com/gpu: $gpus_per_node
          requests:
            cpu: 4
            memory: 16G
            nvidia.com/gpu: $gpus_per_node
        volumes:
          - name: log-volume
            emptyDir: {}
          - name: nfs-shared
            persistentVolumeClaim:
              claimName: pvc-${release_name}
        volumeMounts:
          - mountPath: /tmp/ray
            name: log-volume
          - mountPath: /mnt/nfs
            name: nfs-shared
      worker:
        replicas: $num_nodes
        resources:
          limits:
            cpu: 4
            memory: 16G
            nvidia.com/gpu: $gpus_per_node
          requests:
            cpu: 4
            memory: 16G
            nvidia.com/gpu: $gpus_per_node
        annotations:
          sidecar.istio.io/inject: "false"
        volumes:
          - name: log-volume
            emptyDir: {}
          - name: nfs-shared
            persistentVolumeClaim:
              claimName: pvc-${release_name}
        volumeMounts:
          - mountPath: /tmp/ray
            name: log-volume
          - mountPath: /mnt/nfs
            name: nfs-shared
      service:
        type: NodePort
      EOF

      echo "Rendering Helm templates to YAML..."
      helm template $release_name kuberay/ray-cluster \
        --version 1.3.0 \
        --namespace $namespace \
        -f /tmp/values.yaml > "$rendered_yaml"
    - {inputValue: namespace}
    - {inputValue: release_name}
    - {inputValue: num_nodes}
    - {inputValue: gpus_per_node}
    - {outputPath: rendered_yaml}
