{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4932d433-d4e1-4f9e-9ac9-b6054983b93b",
   "metadata": {},
   "source": [
    " ## Import the necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5eb6b6a-fb37-42ca-b5f4-4125726019ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from kfp.dsl import Input, Model, component, Dataset, Output\n",
    "from kfp.dsl import InputPath, OutputPath, pipeline, component, PipelineTask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b741d7-3bf1-46e9-a14d-4df72f806089",
   "metadata": {},
   "source": [
    "## Ingest your files\n",
    "Create a component that downloads the files and then saves it at a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cbc0d9f-9ed4-4100-ab65-42fd5041e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\"gitpython\"],\n",
    ")\n",
    "def download_files_from_github(output_dir: Output[Dataset]):\n",
    "    import git\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    repo_url = \"https://github.com/ravikiranvs/kf-demo.git\"\n",
    "    target_subdir = \"examples/rag-ingest/files\"\n",
    "\n",
    "    # Clone the repo to /tmp\n",
    "    repo_path = \"/tmp/kf-demo\"\n",
    "    if Path(repo_path).exists():\n",
    "        shutil.rmtree(repo_path)\n",
    "    git.Repo.clone_from(repo_url, repo_path)\n",
    "\n",
    "    # Copy the desired files to the output directory\n",
    "    source_dir = Path(repo_path) / target_subdir\n",
    "    dest_dir = Path(output_dir.path)\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in source_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            shutil.copy(file_path, dest_dir / file_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2a9a1-ec88-45e4-bd19-0457f4c2a11a",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "967b51f1-10d0-4b2c-aa67-8eec9f55d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='downloads.unstructured.io/unstructured-io/unstructured:latest',\n",
    ")\n",
    "def process_pdfs_with_unstructured(input_dir: Input[Dataset], output_structured_data: Output[Dataset]):\n",
    "    import os\n",
    "    import json\n",
    "    from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "    input_path = input_dir.path\n",
    "    output_path = output_structured_data.path\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "            elements = partition_pdf(filename=file_path)\n",
    "            output_file = os.path.join(output_path, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump([element.to_dict() for element in elements], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6601c6-b9a0-4dd8-bfc0-d3df01c48c40",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38acfdc9-4ef9-4701-b694-d5d4b80e93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='huggingface/transformers-pytorch-gpu:latest',\n",
    "    packages_to_install=[\"sentence-transformers\"],\n",
    ")\n",
    "def generate_embeddings(\n",
    "    input_structured_data: Input[Dataset],\n",
    "    output_embeddings: Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    model = SentenceTransformer(\"BAAI/llm-embedder\")\n",
    "\n",
    "    input_path = Path(input_structured_data.path)\n",
    "    output_path = Path(output_embeddings.path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in input_path.glob(\"*.json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            elements = json.load(f)\n",
    "\n",
    "        # Extract text from elements\n",
    "        texts = [el.get(\"text\", \"\") for el in elements if el.get(\"text\")]\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = model.encode(texts)\n",
    "\n",
    "        # Prepare data with metadata\n",
    "        data = []\n",
    "        for idx, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "            data.append({\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": text,\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"source_file\": file.name\n",
    "            })\n",
    "\n",
    "        # Save embeddings to a JSON file\n",
    "        output_file = output_path / f\"{file.stem}_embeddings.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d746bd1-e138-47d0-815c-d34a5b85c15a",
   "metadata": {},
   "source": [
    "## Insert to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ddb672a6-f86b-4622-9eeb-a1037be9a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\"pymilvus\"],\n",
    ")\n",
    "def ingest_embeddings_to_milvus(\n",
    "    input_embeddings: Input[Dataset],\n",
    "    milvus_host: str = 'standalone-milvus.milvus.svc.cluster.local',\n",
    "    milvus_port: int = 19530,\n",
    "    collection_name: str = 'rag_embeddings'\n",
    "):\n",
    "    from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "    import os\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Connect to Milvus\n",
    "    connections.connect(alias=\"default\", host=milvus_host, port=milvus_port)\n",
    "    print(f\"Connected to Milvus at {milvus_host}:{milvus_port}\")\n",
    "\n",
    "    # Check if collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' exists. Dropping it.\")\n",
    "        collection = Collection(name=collection_name)\n",
    "        collection.drop()\n",
    "        print(f\"Collection '{collection_name}' dropped.\")\n",
    "\n",
    "    # Define schema\n",
    "    fields = [\n",
    "        FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "        FieldSchema(name=\"source_file\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description=\"RAG Embeddings Collection\")\n",
    "\n",
    "    # Create collection\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    print(f\"Collection '{collection_name}' created.\")\n",
    "\n",
    "    # Insert data\n",
    "    input_path = Path(input_embeddings.path)\n",
    "    for file in input_path.glob(\"*_embeddings.json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(f\"No data found in {file.name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        chunk_ids = [item[\"chunk_id\"] for item in data]\n",
    "        texts = [item[\"text\"] for item in data]\n",
    "        embeddings = [item[\"embedding\"] for item in data]\n",
    "        source_files = [item[\"source_file\"] for item in data]\n",
    "\n",
    "        entities = [chunk_ids, texts, embeddings, source_files]\n",
    "        collection.insert(entities)\n",
    "        print(f\"Inserted {len(chunk_ids)} records from {file.name} into '{collection_name}'.\")\n",
    "\n",
    "    # Flush the collection to ensure data is persisted\n",
    "    collection.flush()\n",
    "    print(f\"Collection '{collection_name}' flushed.\")\n",
    "\n",
    "    # Create index on the 'embedding' field\n",
    "    index_params = {\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nlist\": 128}\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    print(f\"Index created on 'embedding' field of collection '{collection_name}'.\")\n",
    "\n",
    "    # Load collection into memory\n",
    "    collection.load()\n",
    "    print(f\"Collection '{collection_name}' is loaded and ready for querying.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4fa8e-408a-408d-9e69-8224e93b9c50",
   "metadata": {},
   "source": [
    "## Create a pipeline\n",
    "Create a pipeline that combines all the components you defined in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df2349cf-4d54-4521-bbd1-8c78f0dd370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gpu_request(task: PipelineTask) -> PipelineTask:\n",
    "    \"\"\"Add a request field for a GPU to the container created by the PipelineTask object.\"\"\"\n",
    "    return task.add_node_selector_constraint(accelerator=\"nvidia.com/gpu\").set_accelerator_limit(\n",
    "        limit=1\n",
    "    )\n",
    "\n",
    "@pipeline(name='rag-ingest-pipeline')\n",
    "def rag_ingest_pipeline():\n",
    "    step1 = download_files_from_github()\n",
    "    step2 = process_pdfs_with_unstructured(input_dir=step1.output)\n",
    "    step3 = add_gpu_request(generate_embeddings(input_structured_data=step2.output))\n",
    "    step4 = ingest_embeddings_to_milvus(input_embeddings=step3.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c601e85-6aac-419d-af6b-cabd1505b14a",
   "metadata": {},
   "source": [
    "## Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aae29237-aa05-4580-bc40-2fc4c854147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/eab56b7f-f1ae-4d56-a13b-ae35c83302b6\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/1cf27c31-88c1-4c9f-903b-57ea2a3df49b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "kfp.compiler.Compiler().compile(rag_ingest_pipeline, './pipelines-yaml/rag_ingest_pipeline.yaml')\n",
    "\n",
    "run = client.create_run_from_pipeline_func(rag_ingest_pipeline, arguments={}, enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bb8c0-28bd-4461-8858-2f2dc17d3fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
