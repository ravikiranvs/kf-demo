# PIPELINE DEFINITION
# Name: start-training-job-pipeline
components:
  comp-start-training-job:
    executorLabel: exec-start-training-job
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-start-training-job:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - start_training_job
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'git+https://github.com/kubeflow/trainer.git@master#subdirectory=sdk'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef start_training_job() -> str:\n    from kubeflow.trainer import\
          \ CustomTrainer, TrainerClient\n\n\n    def train_pytorch():\n        import\
          \ os\n\n        import torch\n        from torch import nn\n        import\
          \ torch.nn.functional as F\n\n        from torchvision import datasets,\
          \ transforms\n        import torch.distributed as dist\n        from torch.utils.data\
          \ import DataLoader, DistributedSampler\n\n        # [1] Configure CPU/GPU\
          \ device and distributed backend.\n        # Kubeflow Trainer will automatically\
          \ configure the distributed environment.\n        device, backend = (\"\
          cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n\
          \        dist.init_process_group(backend=backend)\n\n        local_rank\
          \ = int(os.getenv(\"LOCAL_RANK\", 0))\n        print(\n            \"Distributed\
          \ Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.\".format(\n  \
          \              dist.get_world_size(),\n                dist.get_rank(),\n\
          \                local_rank,\n            )\n        )\n\n        # [2]\
          \ Define PyTorch CNN Model to be trained.\n        class Net(nn.Module):\n\
          \            def __init__(self):\n                super(Net, self).__init__()\n\
          \                self.conv1 = nn.Conv2d(1, 20, 5, 1)\n                self.conv2\
          \ = nn.Conv2d(20, 50, 5, 1)\n                self.fc1 = nn.Linear(4 * 4\
          \ * 50, 500)\n                self.fc2 = nn.Linear(500, 10)\n\n        \
          \    def forward(self, x):\n                x = F.relu(self.conv1(x))\n\
          \                x = F.max_pool2d(x, 2, 2)\n                x = F.relu(self.conv2(x))\n\
          \                x = F.max_pool2d(x, 2, 2)\n                x = x.view(-1,\
          \ 4 * 4 * 50)\n                x = F.relu(self.fc1(x))\n               \
          \ x = self.fc2(x)\n                return F.log_softmax(x, dim=1)\n\n  \
          \      # [3] Attach model to the correct device.\n        device = torch.device(f\"\
          {device}:{local_rank}\")\n        model = nn.parallel.DistributedDataParallel(Net().to(device))\n\
          \        model.train()\n        optimizer = torch.optim.SGD(model.parameters(),\
          \ lr=0.1, momentum=0.9)\n\n        # [4] Get the Fashion-MNIST dataset and\
          \ distributed it across all available devices.\n        dataset = datasets.FashionMNIST(\n\
          \            \"./data\",\n            train=True,\n            download=True,\n\
          \            transform=transforms.Compose([transforms.ToTensor()]),\n  \
          \      )\n        train_loader = DataLoader(\n            dataset,\n   \
          \         batch_size=100,\n            sampler=DistributedSampler(dataset),\n\
          \        )\n\n        # [5] Define the training loop.\n        for epoch\
          \ in range(3):\n            for batch_idx, (inputs, labels) in enumerate(train_loader):\n\
          \                # Attach tensors to the device.\n                inputs,\
          \ labels = inputs.to(device), labels.to(device)\n\n                # Forward\
          \ pass\n                outputs = model(inputs)\n                loss =\
          \ F.nll_loss(outputs, labels)\n\n                # Backward pass\n     \
          \           optimizer.zero_grad()\n                loss.backward()\n   \
          \             optimizer.step()\n                if batch_idx % 10 == 0 and\
          \ dist.get_rank() == 0:\n                    print(\n                  \
          \      \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n  \
          \                          epoch,\n                            batch_idx\
          \ * len(inputs),\n                            len(train_loader.dataset),\n\
          \                            100.0 * batch_idx / len(train_loader),\n  \
          \                          loss.item(),\n                        )\n   \
          \                 )\n\n        # Wait for the training to complete and destroy\
          \ to PyTorch distributed process group.\n        dist.barrier()\n      \
          \  if dist.get_rank() == 0:\n            print(\"Training is finished\"\
          )\n        dist.destroy_process_group()\n\n\n    job_id = TrainerClient().train(\n\
          \        trainer=CustomTrainer(\n            func=train_pytorch,\n     \
          \       num_nodes=1,\n            resources_per_node={\n               \
          \ \"cpu\": 5,\n                \"memory\": \"16Gi\",\n                \"\
          gpu\": 1, # Comment this line if you don't have GPUs.\n            },\n\
          \        ),\n        runtime=TrainerClient().get_runtime(\"torch-distributed\"\
          ),\n    )\n\n    print(job_id)\n\n    for s in TrainerClient().get_job(name=job_id).steps:\n\
          \        print(f\"Step: {s.name}, Status: {s.status}, Devices: {s.device}\
          \ x {s.device_count}\")\n\n    return job_id\n\n"
        image: python:3.9
pipelineInfo:
  name: start-training-job-pipeline
root:
  dag:
    tasks:
      start-training-job:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-start-training-job
        taskInfo:
          name: start-training-job
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
